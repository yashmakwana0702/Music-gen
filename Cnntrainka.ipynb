{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"try:\n    import music21\nexcept:\n    !pip install music21\n    !sudo apt-get update\n    !sudo apt install musescore3 -y","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-16T02:22:29.653337Z","iopub.execute_input":"2022-03-16T02:22:29.654844Z","iopub.status.idle":"2022-03-16T02:23:43.207708Z","shell.execute_reply.started":"2022-03-16T02:22:29.654551Z","shell.execute_reply":"2022-03-16T02:23:43.206425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    import midi2audio\nexcept:\n    !pip install midi2audio\n    !sudo apt-get update\n    !sudo apt-get install fluidsynth -y","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-16T02:23:43.210673Z","iopub.execute_input":"2022-03-16T02:23:43.210985Z","iopub.status.idle":"2022-03-16T02:24:18.549182Z","shell.execute_reply.started":"2022-03-16T02:23:43.210941Z","shell.execute_reply":"2022-03-16T02:24:18.548232Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr style=\"border: solid 3px blue;\">\n\n# Introduction\n\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/12/giphy-1.gif)\n\nPicture Credit: https://cdn.analyticsvidhya.com\n\n**What is Musical composition?**\n> Musical composition can refer to an original piece or work of music, either vocal or instrumental, the structure of a musical piece or to the process of creating or writing a new piece of music. People who create new compositions are called composers. Composers of primarily songs are usually called songwriters; with songs, the person who writes lyrics for a song is the lyricist. In many cultures, including Western classical music, the act of composing typically includes the creation of music notation, such as a sheet music \"score,\" which is then performed by the composer or by other musicians. In popular music and traditional music, songwriting may involve the creation of a basic outline of the song, called the lead sheet, which sets out the melody, lyrics and chord progression. In classical music, orchestration (choosing the instruments of a large music ensemble such as an orchestra which will play the different parts of music, such as the melody, accompaniment, countermelody, bassline and so on) is typically done by the composer, but in musical theatre and in pop music, songwriters may hire an arranger to do the orchestration. In some cases, a pop or traditional songwriter may not use written notation at all and instead compose the song in their mind and then play, sing or record it from memory. In jazz and popular music, notable sound recordings by influential performers are given the weight that written or printed scores play in classical music.\n\nRef: https://en.wikipedia.org/wiki/Musical_composition","metadata":{}},{"cell_type":"markdown","source":"Creating something new is a difficult task. Creation does not start from nothing.\nPerhaps it is finding something new on a well-established foundation.\nFrom this perspective, we will be able to create new music through our model.\n\nIn this notebook, we will learn bach's music through RNN and attention and use it to create new music.","metadata":{}},{"cell_type":"markdown","source":"--------------------------\n# Setting Up","metadata":{}},{"cell_type":"code","source":"import IPython\nfrom IPython.display import Image, Audio\nfrom midi2audio import FluidSynth\nfrom music21 import corpus, converter, instrument, note, stream, chord, duration\nimport matplotlib.pyplot as plt\nimport time\n\nimport os\nimport pickle\nimport keras\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.utils.vis_utils import plot_model\n\nimport os\nimport numpy as np\nimport glob\n\nfrom keras.layers import LSTM, Input, Dropout, Dense, Activation, Embedding, Concatenate, Reshape\nfrom keras.layers import Flatten, RepeatVector, Permute, TimeDistributed\nfrom keras.layers import Multiply, Lambda, Softmax\nimport keras.backend as K \nfrom keras.models import Model\nfrom tensorflow.keras.optimizers import RMSprop\n\nfrom keras.utils import np_utils\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-03-16T03:29:09.369229Z","iopub.execute_input":"2022-03-16T03:29:09.369576Z","iopub.status.idle":"2022-03-16T03:29:10.073884Z","shell.execute_reply.started":"2022-03-16T03:29:09.369545Z","shell.execute_reply":"2022-03-16T03:29:10.073055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"--------------------------\n# Let's enjoy the original bach music\n\n![](https://www.dazebaonews.it/media/k2/items/cache/fe1d1389160c973e96c942135c3a5378_XL.jpg)\n\nPicture Credit: https://www.dazebaonews.it/media","metadata":{}},{"cell_type":"markdown","source":"**Chordify Method:**\n> Chordify is a madeup word that we created in music21 for the process of making chords out of non-chords. Chordify powerful tool for reducing a complex score with multiple parts to a succession of chords in one part that represent everything that is happening in the score. \n\nRef: https://web.mit.edu/music21/doc/usersGuide","metadata":{}},{"cell_type":"code","source":"dataset_name = '../input/classical-music-midi/bach'\nfilename = 'bach_846'\nfile = \"{}/{}.mid\".format(dataset_name, filename)\n\noriginal_score = converter.parse(file).chordify()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T02:24:24.936063Z","iopub.execute_input":"2022-03-16T02:24:24.936781Z","iopub.status.idle":"2022-03-16T02:24:31.830087Z","shell.execute_reply.started":"2022-03-16T02:24:24.936732Z","shell.execute_reply":"2022-03-16T02:24:31.829206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's listen to the three performances of bach first.**","metadata":{}},{"cell_type":"code","source":"fs = FluidSynth()\nfile = '../input/classical-music-midi/bach/bach_846.mid'\nfs.midi_to_audio(file, 'bach_846.wav')\nIPython.display.Audio(\"bach_846.wav\") ","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-03-16T02:24:31.832743Z","iopub.execute_input":"2022-03-16T02:24:31.833094Z","iopub.status.idle":"2022-03-16T02:24:45.971576Z","shell.execute_reply.started":"2022-03-16T02:24:31.833058Z","shell.execute_reply":"2022-03-16T02:24:45.970622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fs.midi_to_audio('../input/classical-music-midi/bach/bach_847.mid', 'bach_847.wav')\nIPython.display.Audio(\"bach_847.wav\") ","metadata":{"execution":{"iopub.status.busy":"2022-03-16T02:24:45.973039Z","iopub.execute_input":"2022-03-16T02:24:45.973317Z","iopub.status.idle":"2022-03-16T02:24:58.929719Z","shell.execute_reply.started":"2022-03-16T02:24:45.973287Z","shell.execute_reply":"2022-03-16T02:24:58.928911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fs.midi_to_audio('../input/classical-music-midi/bach/bach_850.mid', 'bach_850.wav')\nIPython.display.Audio(\"bach_850.wav\") ","metadata":{"execution":{"iopub.status.busy":"2022-03-16T02:24:58.931219Z","iopub.execute_input":"2022-03-16T02:24:58.931557Z","iopub.status.idle":"2022-03-16T02:25:10.560738Z","shell.execute_reply.started":"2022-03-16T02:24:58.931529Z","shell.execute_reply":"2022-03-16T02:25:10.559578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_score.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T02:25:10.562304Z","iopub.execute_input":"2022-03-16T02:25:10.562547Z","iopub.status.idle":"2022-03-16T02:25:21.152833Z","shell.execute_reply.started":"2022-03-16T02:25:10.562518Z","shell.execute_reply":"2022-03-16T02:25:21.151866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---------------------\n# Extracting the data\n\nIt loops through the score and extracts the pitch and time of each note (and rest) into two lists. The entire chord is stored as a string, and individual notes in the chord are separated by dots. The male after the name of each note refers to the octave to which the note belongs.","metadata":{}},{"cell_type":"code","source":"notes = []\ndurations = []\n\nfor element in original_score.flat:    \n    if isinstance(element, chord.Chord):\n        notes.append('.'.join(n.nameWithOctave for n in element.pitches))\n        durations.append(element.duration.quarterLength)\n\n    if isinstance(element, note.Note):\n        if element.isRest:\n            notes.append(str(element.name))\n            durations.append(element.duration.quarterLength)\n        else:\n            notes.append(str(element.nameWithOctave))\n            durations.append(element.duration.quarterLength)   ","metadata":{"execution":{"iopub.status.busy":"2022-03-16T02:25:21.202862Z","iopub.execute_input":"2022-03-16T02:25:21.203615Z","iopub.status.idle":"2022-03-16T02:25:21.261033Z","shell.execute_reply.started":"2022-03-16T02:25:21.203567Z","shell.execute_reply":"2022-03-16T02:25:21.260281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('\\nduration', 'pitch')\nidx = 0\nfor n,d in zip(notes,durations):\n    if idx < 50:\n        print(d, '\\t', n)\n    idx = idx + 1   ","metadata":{"execution":{"iopub.status.busy":"2022-03-16T02:25:21.262508Z","iopub.execute_input":"2022-03-16T02:25:21.263038Z","iopub.status.idle":"2022-03-16T02:25:21.288859Z","shell.execute_reply.started":"2022-03-16T02:25:21.262996Z","shell.execute_reply":"2022-03-16T02:25:21.287969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr style=\"border: solid 3px blue;\">\n\n# Creating Music\n\nHere, modeling is done using RNN and atension mechanism, and a new music is composed using this.","metadata":{}},{"cell_type":"markdown","source":"## Defiing Helper Functions","metadata":{}},{"cell_type":"code","source":"def get_music_list(data_folder):    \n    file_list = glob.glob(os.path.join(data_folder, \"*.mid\"))\n    parser = converter    \n    return file_list, parser\n\ndef create_network(n_notes, n_durations, embed_size = 100, rnn_units = 256, use_attention = False):\n    notes_in = Input(shape = (None,))\n    durations_in = Input(shape = (None,))\n\n    x1 = Embedding(n_notes, embed_size)(notes_in)\n    x2 = Embedding(n_durations, embed_size)(durations_in) \n    x = Concatenate()([x1,x2])\n    x = LSTM(rnn_units, return_sequences=True)(x)\n\n    if use_attention:\n        x = LSTM(rnn_units, return_sequences=True)(x)\n        e = Dense(1, activation='tanh')(x)\n        e = Reshape([-1])(e)\n        alpha = Activation('softmax')(e)\n        alpha_repeated = Permute([2, 1])(RepeatVector(rnn_units)(alpha))\n        c = Multiply()([x, alpha_repeated])\n        c = Lambda(lambda xin: K.sum(xin, axis=1), output_shape=(rnn_units,))(c)    \n    else:\n        c = LSTM(rnn_units)(x)\n                                    \n    notes_out = Dense(n_notes, activation = 'softmax', name = 'pitch')(c)\n    durations_out = Dense(n_durations, activation = 'softmax', name = 'duration')(c)\n   \n    model = Model([notes_in, durations_in], [notes_out, durations_out])    \n\n    if use_attention:\n        att_model = Model([notes_in, durations_in], alpha)\n    else:\n        att_model = None\n        \n    opti = RMSprop(lr = 0.001)\n    model.compile(loss=['categorical_crossentropy', 'categorical_crossentropy'], optimizer=opti)\n\n    return model, att_model\n\n\ndef get_distinct(elements):\n    # Get all pitch names\n    element_names = sorted(set(elements))\n    n_elements = len(element_names)\n    return (element_names, n_elements)\n\ndef create_lookups(element_names):\n    # create dictionary to map notes and durations to integers\n    element_to_int = dict((element, number) for number, element in enumerate(element_names))\n    int_to_element = dict((number, element) for number, element in enumerate(element_names))\n    return (element_to_int, int_to_element)    \n\ndef prepare_sequences(notes, durations, lookups, distincts, seq_len =32):\n    note_to_int, int_to_note, duration_to_int, int_to_duration = lookups\n    note_names, n_notes, duration_names, n_durations = distincts\n\n    notes_network_input = []\n    notes_network_output = []\n    durations_network_input = []\n    durations_network_output = []\n\n    # create input sequences and the corresponding outputs\n    for i in range(len(notes) - seq_len):\n        notes_sequence_in = notes[i:i + seq_len]\n        notes_sequence_out = notes[i + seq_len]\n        notes_network_input.append([note_to_int[char] for char in notes_sequence_in])\n        notes_network_output.append(note_to_int[notes_sequence_out])\n\n        durations_sequence_in = durations[i:i + seq_len]\n        durations_sequence_out = durations[i + seq_len]\n        durations_network_input.append([duration_to_int[char] for char in durations_sequence_in])\n        durations_network_output.append(duration_to_int[durations_sequence_out])\n\n    n_patterns = len(notes_network_input)\n\n    # reshape the input into a format compatible with LSTM layers\n    notes_network_input = np.reshape(notes_network_input, (n_patterns, seq_len))\n    durations_network_input = np.reshape(durations_network_input, (n_patterns, seq_len))\n    network_input = [notes_network_input, durations_network_input]\n\n    notes_network_output = np_utils.to_categorical(notes_network_output, num_classes=n_notes)\n    durations_network_output = np_utils.to_categorical(durations_network_output, num_classes=n_durations)\n    network_output = [notes_network_output, durations_network_output]\n    return (network_input, network_output)\n\ndef sample_with_temp(preds, temperature):\n    if temperature == 0:\n        return np.argmax(preds)\n    else:\n        preds = np.log(preds) / temperature\n        exp_preds = np.exp(preds)\n        preds = exp_preds / np.sum(exp_preds)\n        return np.random.choice(len(preds), p=preds)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T02:25:21.290447Z","iopub.execute_input":"2022-03-16T02:25:21.290775Z","iopub.status.idle":"2022-03-16T02:25:21.319037Z","shell.execute_reply.started":"2022-03-16T02:25:21.29074Z","shell.execute_reply":"2022-03-16T02:25:21.318017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run params\nrun_folder = '/kaggle/working'\n\nstore_folder = os.path.join(run_folder, 'store')\ndata_folder ='../input/classical-music-midi/bach'\n\nif not os.path.exists('store'):\n    os.mkdir(os.path.join(run_folder, 'store'))\n    os.mkdir(os.path.join(run_folder, 'output'))\n    os.mkdir(os.path.join(run_folder, 'weights'))\n    os.mkdir(os.path.join(run_folder, 'viz'))\n\nmode = 'build'\n\n# data params\nintervals = range(1)\nseq_len = 32\n\n# model params\nembed_size = 100\nrnn_units = 256\nuse_attention = True","metadata":{"execution":{"iopub.status.busy":"2022-03-16T02:25:21.320618Z","iopub.execute_input":"2022-03-16T02:25:21.320869Z","iopub.status.idle":"2022-03-16T02:25:21.335196Z","shell.execute_reply.started":"2022-03-16T02:25:21.32084Z","shell.execute_reply":"2022-03-16T02:25:21.334395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if mode == 'build':    \n    music_list, parser = get_music_list(data_folder)\n    print(len(music_list), 'files in total')\n\n    notes = []\n    durations = []\n\n    for i, file in enumerate(music_list):\n        print(i+1, \"Parsing %s\" % file)\n        print(file)\n        original_score = parser.parse(file).chordify()        \n        for interval in intervals:\n            score = original_score.transpose(interval)\n\n            notes.extend(['START'] * seq_len)\n            durations.extend([0]* seq_len)\n\n            for element in score.flat:                \n                if isinstance(element, note.Note):\n                    if element.isRest:\n                        notes.append(str(element.name))\n                        durations.append(element.duration.quarterLength)\n                    else:\n                        notes.append(str(element.nameWithOctave))\n                        durations.append(element.duration.quarterLength)\n                        \n                if isinstance(element, chord.Chord):\n                    notes.append('.'.join(n.nameWithOctave for n in element.pitches))\n                    durations.append(element.duration.quarterLength)\n\n    with open(os.path.join(store_folder, 'notes'), 'wb') as f:\n        pickle.dump(notes, f) \n    with open(os.path.join(store_folder, 'durations'), 'wb') as f:\n        pickle.dump(durations, f) \nelse:\n    with open(os.path.join(store_folder, 'notes'), 'rb') as f:\n        notes = pickle.load(f)\n    with open(os.path.join(store_folder, 'durations'), 'rb') as f:\n        durations = pickle.load(f) ","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-16T02:25:21.339234Z","iopub.execute_input":"2022-03-16T02:25:21.339458Z","iopub.status.idle":"2022-03-16T02:25:42.333117Z","shell.execute_reply.started":"2022-03-16T02:25:21.33943Z","shell.execute_reply":"2022-03-16T02:25:42.332393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"------------------------------------------\n# Embedding Note and Duration\n\nTo create a dataset for training the model, we first convert the pitch and tempo into integer values. It doesn't matter what these values are because we use an embedding layer to convert the integer to a vector.","metadata":{}},{"cell_type":"code","source":"# get the distinct sets of notes and durations\nnote_names, n_notes = get_distinct(notes)\nduration_names, n_durations = get_distinct(durations)\ndistincts = [note_names, n_notes, duration_names, n_durations]\n\nwith open(os.path.join(store_folder, 'distincts'), 'wb') as f:\n    pickle.dump(distincts, f)\n\n# make the lookup dictionaries for notes and dictionaries and save\nnote_to_int, int_to_note = create_lookups(note_names)\nduration_to_int, int_to_duration = create_lookups(duration_names)\nlookups = [note_to_int, int_to_note, duration_to_int, int_to_duration]\n\nwith open(os.path.join(store_folder, 'lookups'), 'wb') as f:\n    pickle.dump(lookups, f)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T02:25:42.334175Z","iopub.execute_input":"2022-03-16T02:25:42.334363Z","iopub.status.idle":"2022-03-16T02:25:42.352485Z","shell.execute_reply.started":"2022-03-16T02:25:42.33434Z","shell.execute_reply":"2022-03-16T02:25:42.351542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('\\nnote_to_int')\nfor i, item in enumerate(note_to_int.items()):\n    if i < 10:\n        print(item)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T02:25:42.354298Z","iopub.execute_input":"2022-03-16T02:25:42.354609Z","iopub.status.idle":"2022-03-16T02:25:42.363124Z","shell.execute_reply.started":"2022-03-16T02:25:42.354569Z","shell.execute_reply":"2022-03-16T02:25:42.362388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('\\nduration_to_int')\nduration_to_int","metadata":{"execution":{"iopub.status.busy":"2022-03-16T02:25:42.364494Z","iopub.execute_input":"2022-03-16T02:25:42.365081Z","iopub.status.idle":"2022-03-16T02:25:42.373721Z","shell.execute_reply.started":"2022-03-16T02:25:42.365038Z","shell.execute_reply":"2022-03-16T02:25:42.372962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"network_input, network_output = prepare_sequences(notes, durations, lookups, distincts, seq_len)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T02:25:42.375111Z","iopub.execute_input":"2022-03-16T02:25:42.375862Z","iopub.status.idle":"2022-03-16T02:25:42.680043Z","shell.execute_reply.started":"2022-03-16T02:25:42.37582Z","shell.execute_reply":"2022-03-16T02:25:42.679309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Divide the dataset by 32 notes to create the training set. Target is the next pitch and time signature in the sequence.","metadata":{}},{"cell_type":"code","source":"print('pitch input')\nprint(network_input[0][0])\nprint('duration input')\nprint(network_input[1][0])\nprint('pitch target')\nprint(network_output[0][0])\nprint('duration target')\nprint(network_output[1][0])","metadata":{"execution":{"iopub.status.busy":"2022-03-16T02:25:42.681143Z","iopub.execute_input":"2022-03-16T02:25:42.681358Z","iopub.status.idle":"2022-03-16T02:25:42.691883Z","shell.execute_reply.started":"2022-03-16T02:25:42.681333Z","shell.execute_reply":"2022-03-16T02:25:42.690977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"----------------------------------\n# Modeling","metadata":{}},{"cell_type":"code","source":"model, att_model = create_network(n_notes, n_durations, embed_size, rnn_units, use_attention)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T02:25:42.693012Z","iopub.execute_input":"2022-03-16T02:25:42.693232Z","iopub.status.idle":"2022-03-16T02:25:43.437212Z","shell.execute_reply.started":"2022-03-16T02:25:42.693206Z","shell.execute_reply":"2022-03-16T02:25:43.436379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(model, to_file=os.path.join(run_folder ,'viz/model.png'), show_shapes = True, show_layer_names = True)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T02:25:43.438305Z","iopub.execute_input":"2022-03-16T02:25:43.438506Z","iopub.status.idle":"2022-03-16T02:25:44.400898Z","shell.execute_reply.started":"2022-03-16T02:25:43.43848Z","shell.execute_reply":"2022-03-16T02:25:44.399767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---------------------------------------------\n# Training","metadata":{}},{"cell_type":"code","source":"weights_folder = os.path.join(run_folder, 'weights')\n\ncheckpoint1 = ModelCheckpoint(\n    os.path.join(weights_folder, \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.h5\"),\n    monitor='loss',\n    verbose=0,\n    save_best_only=True,\n    mode='min'\n)\n\ncheckpoint2 = ModelCheckpoint(\n    os.path.join(weights_folder, \"weights.h5\"),\n    monitor='loss',\n    verbose=0,\n    save_best_only=True,\n    mode='min'\n)\n\nearly_stopping = EarlyStopping(\n    monitor='loss'\n    , restore_best_weights=True\n    , patience = 10\n)\n\n\ncallbacks_list = [\n    checkpoint1\n    , checkpoint2\n    , early_stopping\n ]\n\nmodel.save_weights(os.path.join(weights_folder, \"weights.h5\"))\nmodel.fit(network_input, network_output\n          , epochs=2000000, batch_size=32\n          , validation_split = 0.2\n          , callbacks=callbacks_list\n          , shuffle=True\n         )","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-16T02:25:44.402534Z","iopub.execute_input":"2022-03-16T02:25:44.402821Z","iopub.status.idle":"2022-03-16T03:02:11.727564Z","shell.execute_reply.started":"2022-03-16T02:25:44.402771Z","shell.execute_reply":"2022-03-16T03:02:11.726756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"----------------------------------------\n# Predicting","metadata":{}},{"cell_type":"code","source":"# prediction params\nnotes_temp=0.5\nduration_temp = 0.5\nmax_extra_notes = 50\nmax_seq_len = 32\nseq_len = 32\n\nnotes = ['START']\ndurations = [0]\n\nif seq_len is not None:\n    notes = ['START'] * (seq_len - len(notes)) + notes\n    durations = [0] * (seq_len - len(durations)) + durations\n\nsequence_length = len(notes)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T03:02:11.729365Z","iopub.execute_input":"2022-03-16T03:02:11.72986Z","iopub.status.idle":"2022-03-16T03:02:11.740723Z","shell.execute_reply.started":"2022-03-16T03:02:11.729829Z","shell.execute_reply":"2022-03-16T03:02:11.739775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_output = []\nnotes_input_sequence = []\ndurations_input_sequence = []\noverall_preds = []\n\nfor n, d in zip(notes,durations):\n    note_int = note_to_int[n]\n    duration_int = duration_to_int[d]\n    \n    notes_input_sequence.append(note_int)\n    durations_input_sequence.append(duration_int)\n    \n    prediction_output.append([n, d])\n    \n    if n != 'START':\n        midi_note = note.Note(n)\n        new_note = np.zeros(128)\n        new_note[midi_note.pitch.midi] = 1\n        overall_preds.append(new_note)\n\natt_matrix = np.zeros(shape = (max_extra_notes+sequence_length, max_extra_notes))\n\nfor note_index in range(max_extra_notes):\n\n    prediction_input = [\n        np.array([notes_input_sequence])\n        , np.array([durations_input_sequence])\n       ]\n\n    notes_prediction, durations_prediction = model.predict(prediction_input, verbose=0)\n    if use_attention:\n        att_prediction = att_model.predict(prediction_input, verbose=0)[0]\n        att_matrix[(note_index-len(att_prediction)+sequence_length):(note_index+sequence_length), note_index] = att_prediction\n    \n    new_note = np.zeros(128)\n    \n    for idx, n_i in enumerate(notes_prediction[0]):\n        try:\n            note_name = int_to_note[idx]\n            midi_note = note.Note(note_name)\n            new_note[midi_note.pitch.midi] = n_i            \n        except:\n            pass\n        \n    overall_preds.append(new_note)            \n    \n    i1 = sample_with_temp(notes_prediction[0], notes_temp)\n    i2 = sample_with_temp(durations_prediction[0], duration_temp)    \n\n    note_result = int_to_note[i1]\n    duration_result = int_to_duration[i2]\n    \n    prediction_output.append([note_result, duration_result])\n\n    notes_input_sequence.append(i1)\n    durations_input_sequence.append(i2)\n    \n    if len(notes_input_sequence) > max_seq_len:\n        notes_input_sequence = notes_input_sequence[1:]\n        durations_input_sequence = durations_input_sequence[1:]\n        \n    if note_result == 'START':\n        break\n\noverall_preds = np.transpose(np.array(overall_preds)) \nprint('Generated sequence of {} notes'.format(len(prediction_output)))","metadata":{"execution":{"iopub.status.busy":"2022-03-16T03:02:11.742548Z","iopub.execute_input":"2022-03-16T03:02:11.742766Z","iopub.status.idle":"2022-03-16T03:02:23.672571Z","shell.execute_reply.started":"2022-03-16T03:02:11.742739Z","shell.execute_reply":"2022-03-16T03:02:23.671672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---------------------------------------\n# Intrepreting Model","metadata":{}},{"cell_type":"markdown","source":"-------------------------------------------\n## Heatmap","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,15))\nax.set_yticks([int(j) for j in range(35,70)])\nplt.imshow(overall_preds[35:70,:], origin=\"lower\", cmap='coolwarm', vmin = -0.5, vmax = 0.5, extent=[0, max_extra_notes, 35,70])\nplt.xlabel(\"Note number\",fontsize=20)\nplt.ylabel(\"Pitch value (MIDI number)\",fontsize=20)\nplt.title(\"Probability distribution of the next possible note over time\",fontsize=20)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T03:29:35.52443Z","iopub.execute_input":"2022-03-16T03:29:35.525069Z","iopub.status.idle":"2022-03-16T03:29:36.436508Z","shell.execute_reply.started":"2022-03-16T03:29:35.525017Z","shell.execute_reply":"2022-03-16T03:29:36.435568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_folder = os.path.join(run_folder, 'output')\n\nmidi_stream = stream.Stream()\n\n# create note and chord objects based on the values generated by the model\nfor pattern in prediction_output:\n    note_pattern, duration_pattern = pattern\n    # pattern is a chord\n    if ('.' in note_pattern):\n        notes_in_chord = note_pattern.split('.')\n        chord_notes = []\n        for current_note in notes_in_chord:\n            new_note = note.Note(current_note)\n            new_note.duration = duration.Duration(duration_pattern)\n            new_note.storedInstrument = instrument.Violoncello()\n            chord_notes.append(new_note)\n        new_chord = chord.Chord(chord_notes)\n        midi_stream.append(new_chord)\n    elif note_pattern == 'rest':\n    # pattern is a rest\n        new_note = note.Rest()\n        new_note.duration = duration.Duration(duration_pattern)\n        new_note.storedInstrument = instrument.Violoncello()\n        midi_stream.append(new_note)\n    elif note_pattern != 'START':\n    # pattern is a note\n        new_note = note.Note(note_pattern)\n        new_note.duration = duration.Duration(duration_pattern)\n        new_note.storedInstrument = instrument.Violoncello()\n        midi_stream.append(new_note)\n\nmidi_stream = midi_stream.chordify()\ntimestr = time.strftime(\"%Y%m%d-%H%M%S\")\nnew_file = 'output-' + timestr + '.mid'\nmidi_stream.write('midi', fp=os.path.join(output_folder, new_file))","metadata":{"execution":{"iopub.status.busy":"2022-03-16T03:02:24.633955Z","iopub.execute_input":"2022-03-16T03:02:24.634201Z","iopub.status.idle":"2022-03-16T03:02:24.843924Z","shell.execute_reply.started":"2022-03-16T03:02:24.634164Z","shell.execute_reply":"2022-03-16T03:02:24.843343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_path = '/kaggle/working/output/'+new_file\nfs = FluidSynth()\nfs.midi_to_audio(new_path, 'new_output.wav')","metadata":{"execution":{"iopub.status.busy":"2022-03-16T03:02:24.844868Z","iopub.execute_input":"2022-03-16T03:02:24.845595Z","iopub.status.idle":"2022-03-16T03:02:31.864417Z","shell.execute_reply.started":"2022-03-16T03:02:24.845562Z","shell.execute_reply":"2022-03-16T03:02:31.863342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---------------------------------------\n## Attention\n\n![](https://miro.medium.com/max/1400/1*2dLzmSops3jTvTR1wzRX0w.gif)\nPicture Credit: https://miro.medium.com\n\nIn order to determine which notes or sequence of notes may follow a particular passage, it is important to use earlier information far back in the sequence, not the most recent information. A good way to solve this problem is attention. In the attention mechanism, the model builds a context vector by weighting the hidden states in the previous time step of the encoder RNN. The attention mechanism is a set of layers that transforms the encoder's previous and current hidden states into additive weights for context vector generation.","metadata":{}},{"cell_type":"code","source":"if use_attention:\n    fig, ax = plt.subplots(figsize=(20,20))\n    im = ax.imshow(att_matrix[(seq_len-2):,], cmap='coolwarm', interpolation='nearest')    \n\n    # Minor ticks\n    ax.set_xticks(np.arange(-.5, len(prediction_output)- seq_len, 1), minor=True);\n    ax.set_yticks(np.arange(-.5, len(prediction_output)- seq_len, 1), minor=True);\n\n    # Gridlines based on minor ticks\n    ax.grid(which='minor', color='black', linestyle='-', linewidth=1)    \n    \n    # We want to show all ticks...\n    ax.set_xticks(np.arange(len(prediction_output) - seq_len))\n    ax.set_yticks(np.arange(len(prediction_output)- seq_len+2))\n    # ... and label them with the respective list entries\n    ax.set_xticklabels([n[0] for n in prediction_output[(seq_len):]])\n    ax.set_yticklabels([n[0] for n in prediction_output[(seq_len - 2):]])\n    ax.xaxis.tick_top()    \n    plt.setp(ax.get_xticklabels(), rotation=90, ha=\"left\", va = \"center\", rotation_mode=\"anchor\")\n    plt.xlabel(\"sequence of generated notes\",fontsize=20)\n    plt.ylabel(\"The point of attention\",fontsize=20)\n    plt.title(\"The amount of attention given to the network hidden state\",fontsize=30)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T03:48:41.8081Z","iopub.execute_input":"2022-03-16T03:48:41.808438Z","iopub.status.idle":"2022-03-16T03:48:45.149716Z","shell.execute_reply.started":"2022-03-16T03:48:41.808407Z","shell.execute_reply":"2022-03-16T03:48:45.149005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The attention mechanism helps the network determine which of the previous states of the circulating layer is important for successive sequence prediction. The encoder-decoder network predicts the note sequence using the RNN decoder, rather than creating a sequence one note at a time.","metadata":{}},{"cell_type":"markdown","source":"-----------------------------------\n# Let's compare the original performance with the new one.","metadata":{}},{"cell_type":"markdown","source":"---------------------------------------\n## Let's listen originals","metadata":{}},{"cell_type":"code","source":"IPython.display.Audio(\"bach_846.wav\") ","metadata":{"execution":{"iopub.status.busy":"2022-03-16T03:02:35.702732Z","iopub.execute_input":"2022-03-16T03:02:35.705582Z","iopub.status.idle":"2022-03-16T03:02:38.784404Z","shell.execute_reply.started":"2022-03-16T03:02:35.705322Z","shell.execute_reply":"2022-03-16T03:02:38.783252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IPython.display.Audio(\"bach_847.wav\") ","metadata":{"execution":{"iopub.status.busy":"2022-03-16T03:02:38.788053Z","iopub.execute_input":"2022-03-16T03:02:38.788466Z","iopub.status.idle":"2022-03-16T03:02:40.733019Z","shell.execute_reply.started":"2022-03-16T03:02:38.788424Z","shell.execute_reply":"2022-03-16T03:02:40.728941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IPython.display.Audio(\"bach_850.wav\")","metadata":{"execution":{"iopub.status.busy":"2022-03-16T03:02:40.738779Z","iopub.execute_input":"2022-03-16T03:02:40.742871Z","iopub.status.idle":"2022-03-16T03:02:42.404939Z","shell.execute_reply.started":"2022-03-16T03:02:40.742618Z","shell.execute_reply":"2022-03-16T03:02:42.399907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_score.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T03:02:42.41017Z","iopub.execute_input":"2022-03-16T03:02:42.411329Z","iopub.status.idle":"2022-03-16T03:02:57.306915Z","shell.execute_reply.started":"2022-03-16T03:02:42.411205Z","shell.execute_reply":"2022-03-16T03:02:57.305912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_score = converter.parse(new_path).chordify()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T03:02:57.308825Z","iopub.execute_input":"2022-03-16T03:02:57.309097Z","iopub.status.idle":"2022-03-16T03:02:57.43576Z","shell.execute_reply.started":"2022-03-16T03:02:57.309065Z","shell.execute_reply":"2022-03-16T03:02:57.434865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-------------------------------\n## Newly composed music\n\nFinally, let's listen to the music we made with our model.","metadata":{}},{"cell_type":"code","source":"IPython.display.Audio(\"new_output.wav\") ","metadata":{"execution":{"iopub.status.busy":"2022-03-16T03:02:57.437236Z","iopub.execute_input":"2022-03-16T03:02:57.437716Z","iopub.status.idle":"2022-03-16T03:02:57.473984Z","shell.execute_reply.started":"2022-03-16T03:02:57.437671Z","shell.execute_reply":"2022-03-16T03:02:57.472765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wow! Very interesting music was made. It is similar to the previous three songs, but something is different.","metadata":{}},{"cell_type":"code","source":"new_score.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T03:02:57.475394Z","iopub.execute_input":"2022-03-16T03:02:57.475791Z","iopub.status.idle":"2022-03-16T03:02:58.103502Z","shell.execute_reply.started":"2022-03-16T03:02:57.475749Z","shell.execute_reply":"2022-03-16T03:02:58.102826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr style=\"border: solid 3px blue;\">","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}